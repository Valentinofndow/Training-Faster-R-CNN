{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64d30b8b",
   "metadata": {},
   "source": [
    "Import & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99b57632",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa46410",
   "metadata": {},
   "source": [
    "Clean Empty Annotation Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25ced2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QC V2.v6i.yolov11/train/labels: 543 → 543 (cleaned)\n",
      "QC V2.v6i.yolov11/valid/labels: 79 → 79 (cleaned)\n"
     ]
    }
   ],
   "source": [
    "annotation_dirs = [\"QC V2.v6i.yolov11/train/labels\",\n",
    "                   \"QC V2.v6i.yolov11/valid/labels\"]\n",
    "\n",
    "for ann_dir in annotation_dirs:\n",
    "    before = len(os.listdir(ann_dir))\n",
    "\n",
    "    for file in os.listdir(ann_dir):\n",
    "        fpath = os.path.join(ann_dir, file)\n",
    "        if os.path.getsize(fpath) == 0:\n",
    "            os.remove(fpath)\n",
    "\n",
    "    after = len(os.listdir(ann_dir))\n",
    "    print(f\"{ann_dir}: {before} → {after} (cleaned)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427f6241",
   "metadata": {},
   "source": [
    "Custom Dataset (YOLO → Faster R-CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d215d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_dir, label_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # collect annotation files\n",
    "        self.annotations = [f for f in os.listdir(label_dir) if f.endswith(\".txt\")]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ann_name = self.annotations[idx]\n",
    "        img_id = ann_name.replace(\".txt\", \"\")\n",
    "\n",
    "        # find the image file\n",
    "        possible_ext = [\".jpg\", \".jpeg\", \".png\"]\n",
    "        img_path = None\n",
    "\n",
    "        for ext in possible_ext:\n",
    "            if os.path.exists(os.path.join(self.image_dir, img_id + ext)):\n",
    "                img_path = os.path.join(self.image_dir, img_id + ext)\n",
    "                break\n",
    "\n",
    "        if img_path is None:\n",
    "            raise FileNotFoundError(f\"No image found for {img_id}\")\n",
    "\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        height, width, _ = image.shape\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        with open(os.path.join(self.label_dir, ann_name), \"r\") as f:\n",
    "            for line in f:\n",
    "                cls, xc, yc, w, h = map(float, line.strip().split())\n",
    "\n",
    "                cls = int(cls)  # class index stays original\n",
    "                xmin = (xc - w/2) * width\n",
    "                ymin = (yc - h/2) * height\n",
    "                xmax = (xc + w/2) * width\n",
    "                ymax = (yc + h/2) * height\n",
    "\n",
    "                boxes.append([xmin, ymin, xmax, ymax])\n",
    "                labels.append(cls + 1)  # shift class → background = 0\n",
    "\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        image = self.transform(image) if self.transform else transforms.ToTensor()(image)\n",
    "\n",
    "        target = {\"boxes\": boxes, \"labels\": labels}\n",
    "        return image, target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d38984",
   "metadata": {},
   "source": [
    "Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfde00f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(543, 79)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = CustomDataset(\n",
    "    image_dir=\"QC V2.v6i.yolov11/train/images\",\n",
    "    label_dir=\"QC V2.v6i.yolov11/train/labels\",\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "valid_dataset = CustomDataset(\n",
    "    image_dir=\"QC V2.v6i.yolov11/valid/images\",\n",
    "    label_dir=\"QC V2.v6i.yolov11/valid/labels\",\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True,\n",
    "                          collate_fn=lambda x: tuple(zip(*x)))\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False,\n",
    "                          collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "len(train_loader), len(valid_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb30355",
   "metadata": {},
   "source": [
    "Build Faster R-CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9dc40ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 5  # background + 4 classes\n",
    "\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"COCO_V1\")\n",
    "\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(\n",
    "    in_features, num_classes\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.001, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "num_epochs = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d2c71f",
   "metadata": {},
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f11db814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Train Loss: 0.2941\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(loss_dict\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 15\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     18\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "\n",
    "    for images, targets in train_loader:\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        loss = sum(loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "    print(f\"Train Loss: {running_loss / len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7274d0c",
   "metadata": {},
   "source": [
    "Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7978811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"faster_rcnn_custom_4class.pth\")\n",
    "print(\"Model saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7693fa15",
   "metadata": {},
   "source": [
    "Evaluation on Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aecd9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "test_dir = \"Bear_Datasets/test/images\"\n",
    "test_imgs = os.listdir(test_dir)\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 6))\n",
    "ax = ax.ravel()\n",
    "\n",
    "for i in range(2):\n",
    "    img_name = np.random.choice(test_imgs)\n",
    "    img_path = os.path.join(test_dir, img_name)\n",
    "\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = model(img_tensor)[0]\n",
    "\n",
    "    img_np = np.array(img)\n",
    "\n",
    "    ax[i].imshow(img_np)\n",
    "\n",
    "    boxes = pred[\"boxes\"].cpu().numpy()\n",
    "    labels = pred[\"labels\"].cpu().numpy()\n",
    "    scores = pred[\"scores\"].cpu().numpy()\n",
    "\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        if score < 0.7:\n",
    "            continue\n",
    "\n",
    "        xmin, ymin, xmax, ymax = box\n",
    "        rect = patches.Rectangle(\n",
    "            (xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "            linewidth=2, edgecolor=\"red\", facecolor=\"none\"\n",
    "        )\n",
    "        ax[i].add_patch(rect)\n",
    "        ax[i].text(xmin, ymin - 5, f\"class {label-1} ({score:.2f})\",\n",
    "                   color=\"yellow\", fontsize=12)\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
